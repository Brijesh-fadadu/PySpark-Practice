{"cells":[{"cell_type":"markdown","metadata":{"id":"Ezd-Z0gm_yS3"},"source":["# NLP Code Along\n","\n","For this code along we will build a spam filter! We'll use the various NLP tools we learned about as well as a new classifier, Naive Bayes.\n","\n","We'll use a classic dataset for this - UCI Repository SMS Spam Detection: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection"]},{"cell_type":"code","execution_count":41,"metadata":{"collapsed":true,"id":"ph6HkjIz_yS-","executionInfo":{"status":"ok","timestamp":1657121994714,"user_tz":-330,"elapsed":454,"user":{"displayName":"Brijesh Fadadu","userId":"17045474612112177029"}}},"outputs":[],"source":["from pyspark.sql import SparkSession"]},{"cell_type":"code","execution_count":42,"metadata":{"collapsed":true,"id":"KlCPY85l_yTA","executionInfo":{"status":"ok","timestamp":1657121995166,"user_tz":-330,"elapsed":9,"user":{"displayName":"Brijesh Fadadu","userId":"17045474612112177029"}}},"outputs":[],"source":["spark = SparkSession.builder.appName('nlp').getOrCreate()"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"upDVRUoM_yTB","executionInfo":{"status":"ok","timestamp":1657121995898,"user_tz":-330,"elapsed":740,"user":{"displayName":"Brijesh Fadadu","userId":"17045474612112177029"}}},"outputs":[],"source":["data = spark.read.csv(\"/content/drive/MyDrive/Spark with Python/Python-and-Spark-for-Big-Data-master/Spark_for_Machine_Learning/Natural_Language_Processing/smsspamcollection\",inferSchema=True,sep='\\t')"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"3azn9CF8_yTC","executionInfo":{"status":"ok","timestamp":1657121996442,"user_tz":-330,"elapsed":547,"user":{"displayName":"Brijesh Fadadu","userId":"17045474612112177029"}}},"outputs":[],"source":["data = data.withColumnRenamed('_c0','class').withColumnRenamed('_c1','text')"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J8t7iyxe_yTD","executionInfo":{"status":"ok","timestamp":1657121996443,"user_tz":-330,"elapsed":12,"user":{"displayName":"Brijesh Fadadu","userId":"17045474612112177029"}},"outputId":"74fd9306-df5c-4841-9c3c-f601c84ab59e"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+--------------------+\n","|class|                text|\n","+-----+--------------------+\n","|  ham|Go until jurong p...|\n","|  ham|Ok lar... Joking ...|\n","| spam|Free entry in 2 a...|\n","|  ham|U dun say so earl...|\n","|  ham|Nah I don't think...|\n","| spam|FreeMsg Hey there...|\n","|  ham|Even my brother i...|\n","|  ham|As per your reque...|\n","| spam|WINNER!! As a val...|\n","| spam|Had your mobile 1...|\n","|  ham|I'm gonna be home...|\n","| spam|SIX chances to wi...|\n","| spam|URGENT! You have ...|\n","|  ham|I've been searchi...|\n","|  ham|I HAVE A DATE ON ...|\n","| spam|XXXMobileMovieClu...|\n","|  ham|Oh k...i'm watchi...|\n","|  ham|Eh u remember how...|\n","|  ham|Fine if thats th...|\n","| spam|England v Macedon...|\n","+-----+--------------------+\n","only showing top 20 rows\n","\n"]}],"source":["data.show()"]},{"cell_type":"markdown","metadata":{"id":"ovjEqlKf_yTF"},"source":["## Clean and Prepare the Data"]},{"cell_type":"markdown","metadata":{"id":"WTvuv0_m_yTF"},"source":["** Create a new length feature: **"]},{"cell_type":"code","execution_count":46,"metadata":{"collapsed":true,"id":"VfQt3kJh_yTN","executionInfo":{"status":"ok","timestamp":1657121996444,"user_tz":-330,"elapsed":10,"user":{"displayName":"Brijesh Fadadu","userId":"17045474612112177029"}}},"outputs":[],"source":["from pyspark.sql.functions import length"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"n5n74YkP_yTO","executionInfo":{"status":"ok","timestamp":1657121996444,"user_tz":-330,"elapsed":8,"user":{"displayName":"Brijesh Fadadu","userId":"17045474612112177029"}}},"outputs":[],"source":["data = data.withColumn('length',length(data['text']))"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DgfTWvWW_yTQ","executionInfo":{"status":"ok","timestamp":1657121996921,"user_tz":-330,"elapsed":485,"user":{"displayName":"Brijesh Fadadu","userId":"17045474612112177029"}},"outputId":"d97eadb8-e7fc-413e-b640-310239cb575a"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+--------------------+------+\n","|class|                text|length|\n","+-----+--------------------+------+\n","|  ham|Go until jurong p...|   111|\n","|  ham|Ok lar... Joking ...|    29|\n","| spam|Free entry in 2 a...|   155|\n","|  ham|U dun say so earl...|    49|\n","|  ham|Nah I don't think...|    61|\n","| spam|FreeMsg Hey there...|   147|\n","|  ham|Even my brother i...|    77|\n","|  ham|As per your reque...|   160|\n","| spam|WINNER!! As a val...|   157|\n","| spam|Had your mobile 1...|   154|\n","|  ham|I'm gonna be home...|   109|\n","| spam|SIX chances to wi...|   136|\n","| spam|URGENT! You have ...|   155|\n","|  ham|I've been searchi...|   196|\n","|  ham|I HAVE A DATE ON ...|    35|\n","| spam|XXXMobileMovieClu...|   149|\n","|  ham|Oh k...i'm watchi...|    26|\n","|  ham|Eh u remember how...|    81|\n","|  ham|Fine if thats th...|    56|\n","| spam|England v Macedon...|   155|\n","+-----+--------------------+------+\n","only showing top 20 rows\n","\n"]}],"source":["data.show()"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DsyZhbg0_yTQ","executionInfo":{"status":"ok","timestamp":1657121997391,"user_tz":-330,"elapsed":473,"user":{"displayName":"Brijesh Fadadu","userId":"17045474612112177029"}},"outputId":"08e03a05-db43-4e17-8f0c-d7fa77183bc4"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------+-----------------+\n","|               class|      avg(length)|\n","+--------------------+-----------------+\n","|                 ham|71.45431945307645|\n","|                spam|138.6706827309237|\n","|We would apprecia...|             null|\n","|spam   URGENT! Yo...|             null|\n","|1. Tiago Agostinh...|             null|\n","|3. Limitation of ...|             null|\n","|ham   Cos i was o...|             null|\n","|    1.1. Compilation|             null|\n","|      1. DESCRIPTION|             null|\n","|The SMS Spam Coll...|             null|\n","|Note: messages ar...|             null|\n","|We offer a compre...|             null|\n","|-----------------...|             null|\n","|         1.3. Format|             null|\n","|            --------|             null|\n","|- A collection of...|             null|\n","|The files contain...|             null|\n","|The corpus has be...|             null|\n","|2. No Warranty/Us...|             null|\n","|ham   Siva is in ...|             null|\n","+--------------------+-----------------+\n","only showing top 20 rows\n","\n"]}],"source":["# Pretty Clear Difference\n","data.groupby('class').mean().show()"]},{"cell_type":"markdown","metadata":{"id":"wtRafUNW_yTR"},"source":["## Feature Transformations"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"NIh0Jmcn_yTU","executionInfo":{"status":"ok","timestamp":1657121997391,"user_tz":-330,"elapsed":10,"user":{"displayName":"Brijesh Fadadu","userId":"17045474612112177029"}}},"outputs":[],"source":["from pyspark.ml.feature import Tokenizer,StopWordsRemover, CountVectorizer,IDF,StringIndexer\n","\n","tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n","stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n","count_vec = CountVectorizer(inputCol='stop_tokens',outputCol='c_vec')\n","idf = IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")\n","ham_spam_to_num = StringIndexer(inputCol='class',outputCol='label')"]},{"cell_type":"code","execution_count":51,"metadata":{"collapsed":true,"id":"7grnawiQ_yTU","executionInfo":{"status":"ok","timestamp":1657121997392,"user_tz":-330,"elapsed":11,"user":{"displayName":"Brijesh Fadadu","userId":"17045474612112177029"}}},"outputs":[],"source":["from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.linalg import Vector"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"J-SHHpJp_yTV","executionInfo":{"status":"ok","timestamp":1657121997393,"user_tz":-330,"elapsed":11,"user":{"displayName":"Brijesh Fadadu","userId":"17045474612112177029"}}},"outputs":[],"source":["clean_up = VectorAssembler(inputCols=['tf_idf','length'],outputCol='features')"]},{"cell_type":"markdown","metadata":{"id":"-jyNYpOZ_yTV"},"source":["### The Model\n","\n","We'll use Naive Bayes, but feel free to play around with this choice!"]},{"cell_type":"code","execution_count":53,"metadata":{"collapsed":true,"id":"Xa-YweMI_yTV","executionInfo":{"status":"ok","timestamp":1657121997393,"user_tz":-330,"elapsed":10,"user":{"displayName":"Brijesh Fadadu","userId":"17045474612112177029"}}},"outputs":[],"source":["from pyspark.ml.classification import NaiveBayes"]},{"cell_type":"code","execution_count":54,"metadata":{"collapsed":true,"id":"3BZeUeY2_yTW","executionInfo":{"status":"ok","timestamp":1657121997393,"user_tz":-330,"elapsed":10,"user":{"displayName":"Brijesh Fadadu","userId":"17045474612112177029"}}},"outputs":[],"source":["# Use defaults\n","nb = NaiveBayes()"]},{"cell_type":"markdown","metadata":{"id":"WfKCiyVu_yTW"},"source":["### Pipeline"]},{"cell_type":"code","execution_count":55,"metadata":{"collapsed":true,"id":"aswErWPD_yTW","executionInfo":{"status":"ok","timestamp":1657121997394,"user_tz":-330,"elapsed":10,"user":{"displayName":"Brijesh Fadadu","userId":"17045474612112177029"}}},"outputs":[],"source":["from pyspark.ml import Pipeline"]},{"cell_type":"code","execution_count":56,"metadata":{"collapsed":true,"id":"DTkMkvFJ_yTW","executionInfo":{"status":"ok","timestamp":1657121997394,"user_tz":-330,"elapsed":8,"user":{"displayName":"Brijesh Fadadu","userId":"17045474612112177029"}}},"outputs":[],"source":["data_prep_pipe = Pipeline(stages=[ham_spam_to_num,tokenizer,stopremove,count_vec,idf,clean_up])"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"AeXZ31vG_yTX","executionInfo":{"status":"error","timestamp":1657121999279,"user_tz":-330,"elapsed":1893,"user":{"displayName":"Brijesh Fadadu","userId":"17045474612112177029"}},"outputId":"f3edac73-6023-4d9e-e878-ef47a10f75d8"},"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-57-34448dcf0531>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcleaner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_prep_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o290.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 34.0 failed 1 times, most recent failure: Lost task 1.0 in stage 34.0 (TID 38) (29282042c777 executor driver): org.apache.spark.SparkException: Failed to execute user defined function (Tokenizer$$Lambda$3722/0x00000008414dc040: (string) => array<string>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.feature.Tokenizer.$anonfun$createTransformFunc$1(Tokenizer.scala:40)\n\t... 19 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1274)\n\tat org.apache.spark.ml.feature.CountVectorizer.fit(CountVectorizer.scala:233)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (Tokenizer$$Lambda$3722/0x00000008414dc040: (string) => array<string>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.feature.Tokenizer.$anonfun$createTransformFunc$1(Tokenizer.scala:40)\n\t... 19 more\n"]}],"source":["cleaner = data_prep_pipe.fit(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nyrRPx02_yTX","executionInfo":{"status":"aborted","timestamp":1657121999270,"user_tz":-330,"elapsed":495,"user":{"displayName":"Brijesh Fadadu","userId":"17045474612112177029"}}},"outputs":[],"source":["clean_data = cleaner.transform(data)"]},{"cell_type":"markdown","metadata":{"id":"mPX_sTBD_yTY"},"source":["### Training and Evaluation!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7NtfqgQZ_yTY","executionInfo":{"status":"aborted","timestamp":1657121999272,"user_tz":-330,"elapsed":496,"user":{"displayName":"Brijesh Fadadu","userId":"17045474612112177029"}}},"outputs":[],"source":["clean_data = clean_data.select(['label','features'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DX9WR3Yc_yTY","executionInfo":{"status":"aborted","timestamp":1657121999273,"user_tz":-330,"elapsed":497,"user":{"displayName":"Brijesh Fadadu","userId":"17045474612112177029"}}},"outputs":[],"source":["clean_data.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"G0kJcG5i_yTY","executionInfo":{"status":"aborted","timestamp":1657121999274,"user_tz":-330,"elapsed":498,"user":{"displayName":"Brijesh Fadadu","userId":"17045474612112177029"}}},"outputs":[],"source":["(training,testing) = clean_data.randomSplit([0.7,0.3])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nkFA5Jbg_yTY","executionInfo":{"status":"aborted","timestamp":1657121999275,"user_tz":-330,"elapsed":499,"user":{"displayName":"Brijesh Fadadu","userId":"17045474612112177029"}}},"outputs":[],"source":["spam_predictor = nb.fit(training)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ow3g_Muy_yTZ","executionInfo":{"status":"aborted","timestamp":1657121999275,"user_tz":-330,"elapsed":499,"user":{"displayName":"Brijesh Fadadu","userId":"17045474612112177029"}}},"outputs":[],"source":["data.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"yFHci_5T_yTa","executionInfo":{"status":"aborted","timestamp":1657121999276,"user_tz":-330,"elapsed":500,"user":{"displayName":"Brijesh Fadadu","userId":"17045474612112177029"}}},"outputs":[],"source":["test_results = spam_predictor.transform(testing)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EvMaJh7X_yTa","executionInfo":{"status":"aborted","timestamp":1657121999277,"user_tz":-330,"elapsed":501,"user":{"displayName":"Brijesh Fadadu","userId":"17045474612112177029"}}},"outputs":[],"source":["test_results.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"--OnvyWw_yTa","executionInfo":{"status":"aborted","timestamp":1657121999278,"user_tz":-330,"elapsed":16,"user":{"displayName":"Brijesh Fadadu","userId":"17045474612112177029"}}},"outputs":[],"source":["from pyspark.ml.evaluation import MulticlassClassificationEvaluator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vGcd8Pqd_yTb","executionInfo":{"status":"aborted","timestamp":1657121999278,"user_tz":-330,"elapsed":16,"user":{"displayName":"Brijesh Fadadu","userId":"17045474612112177029"}}},"outputs":[],"source":["acc_eval = MulticlassClassificationEvaluator()\n","acc = acc_eval.evaluate(test_results)\n","print(\"Accuracy of model at predicting spam was: {}\".format(acc))"]},{"cell_type":"markdown","metadata":{"id":"jzlsitsI_yTb"},"source":["Not bad considering we're using straight math on text data! Try switching out the classification models! Or even try to come up with other engineered features!\n","\n","## Great Job!"]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python [conda root]","language":"python","name":"conda-root-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.3"},"colab":{"name":"Spark_NLP_SMS_Spam_Detection.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}